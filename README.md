Learning Probability Density Function Using Generative Adversarial Networks

1. Introduction

In many real-world scenarios, the underlying probability distribution of a random variable is unknown and cannot be expressed analytically. This project focuses on learning an unknown probability density function (PDF) directly from data samples using a Generative Adversarial Network (GAN).

The task is performed on a transformed version of NO2 concentration data obtained from the India Air Quality dataset. The GAN learns the distribution using only samples, without assuming any parametric or analytical form.

----------------------------------------------------------------

2. Dataset Description

Dataset Name: India Air Quality Data
Source: Kaggle
Dataset Link: https://www.kaggle.com/datasets/shrutibhargava94/india-air-quality-data

Selected Feature:
NO2 (Nitrogen Dioxide concentration)

The dataset is uploaded manually to Google Colab in ZIP format and extracted during execution.

----------------------------------------------------------------

3. Data Transformation

Each value of the NO2 concentration x is transformed using the following nonlinear function:

z = x + a_r sin(b_r x)

where:
a_r = 0.5 × (r mod 7)
b_r = 0.3 × ((r mod 5) + 1)
r is the University Roll Number

After transformation, the variable z is normalized to zero mean and unit variance for stable GAN training.

----------------------------------------------------------------

4. Problem Statement

Given samples of a transformed random variable z, the objective is to:
- Learn the unknown probability density function of z
- Use only sample data without assuming any analytical or parametric PDF
- Approximate the PDF using samples generated by a GAN

----------------------------------------------------------------

5. GAN Architecture

Generator Network:
The generator maps noise sampled from a standard normal distribution N(0,1) to realistic samples of z using a fully connected neural network with ReLU activations.

Discriminator Network:
The discriminator classifies input samples as real or fake using a fully connected neural network with LeakyReLU activations and a sigmoid output.

----------------------------------------------------------------

6. Training Methodology

Loss Function: Binary Cross-Entropy Loss
Optimizer: Adam
Learning Rate: 0.0002
Batch Size: 128
Epochs: 3000
Framework: PyTorch

The generator and discriminator are trained in an adversarial manner. The discriminator learns to distinguish real and fake samples, while the generator learns to fool the discriminator.

----------------------------------------------------------------

7. PDF Estimation

After training:
- A large number of samples are generated using the trained generator
- Kernel Density Estimation (KDE) is applied to estimate the probability density
- KDE plots of real and generated samples are compared

----------------------------------------------------------------

8. Results and Observations

Mode Coverage:
The GAN successfully captures the main modes of the transformed NO2 distribution.

Training Stability:
Normalization improves convergence and stable training behavior. No severe mode collapse is observed.

Quality of Generated Distribution:
The KDE plots of real and generated samples show significant overlap, indicating a good approximation of the underlying PDF.

----------------------------------------------------------------

9. Tools and Technologies Used

Python
PyTorch
Pandas
NumPy
Matplotlib
Seaborn
Google Colab

----------------------------------------------------------------

10. How to Run

1. Download the dataset ZIP file from Kaggle
2. Upload the ZIP file to Google Colab
3. Run the provided Python code
4. View the KDE plot comparing real and generated distributions

----------------------------------------------------------------

11. Conclusion

This project demonstrates that Generative Adversarial Networks can effectively learn unknown probability density functions using data alone, without making any parametric assumptions. The results validate GANs as a powerful tool for implicit density estimation.

----------------------------------------------------------------

12. Future Work

- Extension to multidimensional air quality features
- Use of Wasserstein GANs for improved stability
- Quantitative evaluation using divergence measures
